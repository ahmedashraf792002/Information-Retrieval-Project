{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a7dde06-c19d-40fb-a156-1f4487801968",
   "metadata": {},
   "source": [
    "                        inverted index For Arabic and English Documents \n",
    "\n",
    "Arabic Steps PreProcessing:\n",
    "\n",
    "1.word tokenize\n",
    "\n",
    "2.remove stopwords\n",
    "\n",
    "3.stemming\n",
    "\n",
    "    3.1 ISRIStemmer\n",
    "    3.2 SnowballStemmer\n",
    "\n",
    "4.WordNetLemmatizer\n",
    "\n",
    "English Steps Preprocessing:\n",
    "\n",
    "1.word tokenize\n",
    "\n",
    "2.remove stopwords\n",
    "\n",
    "3.stemming\n",
    "\n",
    "    3.1 PorterStemmer\n",
    "    3.2 SnowballStemmer\n",
    "4.WordNetLemmatizer\n",
    "\n",
    "\n",
    "End Applay Inverted Index For Arabic Or English Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fc52857-b74b-4769-8e26-6e12f7785479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ahmed\n",
      "[nltk_data]     Ashraf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Ahmed\n",
      "[nltk_data]     Ashraf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer,ISRIStemmer,PorterStemmer,SnowballStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9141e7-aa52-4c8b-ba68-19d57e107b76",
   "metadata": {},
   "source": [
    "                    Arabic and English Steps PreProcessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649835f9-8752-4db3-b7ad-8c8306d0dc6d",
   "metadata": {},
   "source": [
    "word tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b557b405-d3e7-4a3b-99f8-e14ae653706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(document,flage=1):\n",
    "    '''\n",
    "    flage==1 mean tokenize using word_tokenize\n",
    "    flage==0 mean tokenize using split\n",
    "    document input doctument text\n",
    "    '''\n",
    "    if flage:\n",
    "        return word_tokenize(document)\n",
    "    #mean else    \n",
    "    return document.split()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2ebfa7b-7fe0-45f8-9708-89e6524fc0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Word Tokenize\n",
      "['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', '.']\n",
      "['احمد', 'اشرف', 'احمد', 'السيد', 'على', 'ليدر', 'مشروع', 'التخرج']\n",
      "**********\n",
      "Using Split\n",
      "['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog.']\n",
      "['احمد', 'اشرف', 'احمد', 'السيد', 'على', 'ليدر', 'مشروع', 'التخرج']\n"
     ]
    }
   ],
   "source": [
    "document1 = \"The quick brown fox jumped over the lazy dog.\"\n",
    "document2 = \"احمد اشرف احمد السيد على ليدر مشروع التخرج\"\n",
    "print('Using Word Tokenize')\n",
    "print(tokenize(document1))\n",
    "print(tokenize(document2))\n",
    "print('*'*10)\n",
    "print('Using Split')\n",
    "print(tokenize(document1,flage=0))\n",
    "print(tokenize(document2,flage=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae582019-9d8f-4004-a68f-da2ca230c9d1",
   "metadata": {},
   "source": [
    "remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6e50c49-021b-4000-b765-fc0cb5a7cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StopWords_Remove(document_tokenize,flage=1):\n",
    "    '''\n",
    "    flage==1 english document\n",
    "    flage==0 arabic document\n",
    "    document_tokenize mean document performed tokenize\n",
    "    '''\n",
    "    if flage:\n",
    "        english_stop=set(stopwords.words('english'))\n",
    "        return [i for i in document_tokenize if i not in english_stop]\n",
    "    # mean else    \n",
    "    arabic_stop=set(stopwords.words('arabic'))\n",
    "    return [i for i in document_tokenize if i not in arabic_stop]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34a69223-fc18-436d-9b49-9e670c7ea24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Word Tokenize & Remove StopWord\n",
      "['The', 'quick', 'brown', 'fox', 'jumped', 'lazy', 'dog', '.']\n",
      "['احمد', 'اشرف', 'احمد', 'السيد', 'ليدر', 'مشروع', 'التخرج']\n",
      "**********\n",
      "Using Split & Remove StopWord\n",
      "['The', 'quick', 'brown', 'fox', 'jumped', 'lazy', 'dog.']\n",
      "['احمد', 'اشرف', 'احمد', 'السيد', 'ليدر', 'مشروع', 'التخرج']\n"
     ]
    }
   ],
   "source": [
    "document1 = \"The quick brown fox jumped over the lazy dog.\"\n",
    "document2 = \"احمد اشرف احمد السيد على ليدر مشروع التخرج\"\n",
    "print('Using Word Tokenize & Remove StopWord')\n",
    "print(StopWords_Remove(tokenize(document1)))\n",
    "print(StopWords_Remove(tokenize(document2),flage=0))\n",
    "print('*'*10)\n",
    "print('Using Split & Remove StopWord')\n",
    "print(StopWords_Remove(tokenize(document1,flage=0)))\n",
    "print(StopWords_Remove(tokenize(document2,flage=0),flage=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006b6d5a-59c0-4423-ada8-b727a46107d0",
   "metadata": {},
   "source": [
    "stemming & Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d925a7d-4ef9-454c-9669-0a8bfca0ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_arabic(document_tokenize,flage=1):\n",
    "    '''\n",
    "    flage==1 mean perform ISRIStemmer\n",
    "    flage==2 mean perform SnowballStemmer\n",
    "    flage==3 mean perform WordNetLemmatizer\n",
    "    document_tokenize mean document performed tokenize and may be performed stopword remove\n",
    "    '''\n",
    "    if flage==1:\n",
    "        stem=ISRIStemmer()\n",
    "        return [stem.stem(i) for i in document_tokenize]\n",
    "    elif flage==2:\n",
    "        stem=SnowballStemmer('arabic')\n",
    "        return [stem.stem(i) for i in document_tokenize]\n",
    "    else:\n",
    "        #Lemmatizer of each word\n",
    "        lemmatizer=WordNetLemmatizer()\n",
    "        return [lemmatizer.lemmatize(i) for i in document_tokenize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d969e91-988e-457b-b3e5-8d4f02c4db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_english(document_tokenize,flage=1):\n",
    "    '''\n",
    "    flage==1 mean perform PorterStemmer\n",
    "    flage==2 mean perform SnowballStemmer\n",
    "    flage==3 mean perform WordNetLemmatizer\n",
    "    document_tokenize mean document performed tokenize and may be performed stopword remove\n",
    "    '''\n",
    "    if flage==1:\n",
    "       stem=PorterStemmer()\n",
    "       return [stem.stem(i) for i in document_tokenize]\n",
    "    elif flage==2:\n",
    "        stem=SnowballStemmer('english')\n",
    "        return [stem.stem(i) for i in document_tokenize]\n",
    "    else:\n",
    "        #Lemmatizer of each word\n",
    "        lemmatizer=WordNetLemmatizer()\n",
    "        return [lemmatizer.lemmatize(i) for i in document_tokenize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a3cc834-45a5-4f36-8f12-01c17f46463b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Word Tokenize & Remove StopWord & Stemming\n",
      "['the', 'quick', 'brown', 'fox', 'jump', 'lazi', 'dog', '.']\n",
      "['حمد', 'شرف', 'حمد', 'سيد', 'يدر', 'شرع', 'خرج']\n",
      "**********\n",
      "Using Split & Remove StopWord & Stemming\n",
      "['the', 'quick', 'brown', 'fox', 'jump', 'lazi', 'dog.']\n",
      "['حمد', 'شرف', 'حمد', 'سيد', 'يدر', 'شرع', 'خرج']\n"
     ]
    }
   ],
   "source": [
    "document1 = \"The quick brown fox jumped over the lazy dog.\"\n",
    "document2 = \"احمد اشرف احمد السيد على ليدر مشروع التخرج\"\n",
    "print('Using Word Tokenize & Remove StopWord & Stemming')\n",
    "print(stemming_english(StopWords_Remove(tokenize(document1))))\n",
    "print(stemming_arabic(StopWords_Remove(tokenize(document2),flage=0)))\n",
    "print('*'*10)\n",
    "print('Using Split & Remove StopWord & Stemming')\n",
    "print(stemming_english(StopWords_Remove(tokenize(document1,flage=0))))\n",
    "print(stemming_arabic(StopWords_Remove(tokenize(document2,flage=0),flage=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36453667-33ce-42ee-8a94-3c6a74c02177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Word Tokenize & Remove StopWord & Stemming\n",
      "['the', 'quick', 'brown', 'fox', 'jump', 'lazi', 'dog', '.']\n",
      "['احمد', 'اشرف', 'احمد', 'سيد', 'ليدر', 'مشروع', 'تخرج']\n",
      "**********\n",
      "Using Split & Remove StopWord & Stemming\n",
      "['the', 'quick', 'brown', 'fox', 'jump', 'lazi', 'dog.']\n",
      "['احمد', 'اشرف', 'احمد', 'سيد', 'ليدر', 'مشروع', 'تخرج']\n"
     ]
    }
   ],
   "source": [
    "document1 = \"The quick brown fox jumped over the lazy dog.\"\n",
    "document2 = \"احمد اشرف احمد السيد على ليدر مشروع التخرج\"\n",
    "print('Using Word Tokenize & Remove StopWord & Stemming')\n",
    "print(stemming_english(StopWords_Remove(tokenize(document1)),flage=2))\n",
    "print(stemming_arabic(StopWords_Remove(tokenize(document2),flage=0),flage=2))\n",
    "print('*'*10)\n",
    "print('Using Split & Remove StopWord & Stemming')\n",
    "print(stemming_english(StopWords_Remove(tokenize(document1,flage=0)),flage=2))\n",
    "print(stemming_arabic(StopWords_Remove(tokenize(document2,flage=0),flage=0),flage=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14015a66-e50d-43d7-a6c1-33c5383288bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Word Tokenize & Remove StopWord & Stemming\n",
      "['The', 'quick', 'brown', 'fox', 'jumped', 'lazy', 'dog', '.']\n",
      "['احمد', 'اشرف', 'احمد', 'السيد', 'ليدر', 'مشروع', 'التخرج']\n",
      "**********\n",
      "Using Split & Remove StopWord & Stemming\n",
      "['The', 'quick', 'brown', 'fox', 'jumped', 'lazy', 'dog.']\n",
      "['احمد', 'اشرف', 'احمد', 'السيد', 'ليدر', 'مشروع', 'التخرج']\n"
     ]
    }
   ],
   "source": [
    "document1 = \"The quick brown fox jumped over the lazy dog.\"\n",
    "document2 = \"احمد اشرف احمد السيد على ليدر مشروع التخرج\"\n",
    "print('Using Word Tokenize & Remove StopWord & Stemming')\n",
    "print(stemming_english(StopWords_Remove(tokenize(document1)),flage=3))\n",
    "print(stemming_arabic(StopWords_Remove(tokenize(document2),flage=0),flage=3))\n",
    "print('*'*10)\n",
    "print('Using Split & Remove StopWord & Stemming')\n",
    "print(stemming_english(StopWords_Remove(tokenize(document1,flage=0)),flage=3))\n",
    "print(stemming_arabic(StopWords_Remove(tokenize(document2,flage=0),flage=0),flage=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5850d40e-a93d-4e23-a341-975a7c422ad8",
   "metadata": {},
   "source": [
    "                End Applay Inverted Index For Arabic Or English Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d98fca2-7a7f-4a3d-9959-cc6b33b9493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def InvertedIndex(*document, text_flage=1, tokenize_flage=1, bool_apply_stopword=1, stopword_flage=1, bool_apply_stemming=1, stemming_flage=1):\n",
    "    '''\n",
    "    *document: must be dict that name is key and doc text is value\n",
    "    tokenize_flage: must be 1 word_tokenize or 0 split \n",
    "    text_flage: must be 1 english or 0 arabic\n",
    "    bool_apply_stopword: must be 1 apply or 0 don't apply\n",
    "    stopword_flage: must be 1 english or 0 arabic\n",
    "    bool_apply_stemming: must be 1 apply or 0 don't apply\n",
    "    stemming_flage: must be 1 or 2 or 3 and depends on text_flag arabic or english\n",
    "    '''\n",
    "    inverted_index = {}\n",
    "    inverted_index2 = {}\n",
    "    terms = set([])\n",
    "    names = []\n",
    "    document_t = []\n",
    "    for i in document:\n",
    "        for i2 in i.items():\n",
    "            names.append(i2[0])\n",
    "            text = tokenize(i2[1], tokenize_flage)\n",
    "            if bool_apply_stopword:\n",
    "                text = StopWords_Remove(text, text_flage)\n",
    "            if bool_apply_stemming:\n",
    "                if text_flage:\n",
    "                    stemming_english(text, stemming_flage)\n",
    "                else:\n",
    "                    stemming_arabic(text, stemming_flage)\n",
    "        terms.update(text)\n",
    "        document_t.append(text)\n",
    "\n",
    "    for term in terms:\n",
    "        document1 = []\n",
    "        document2 = []\n",
    "        for document_m in range(len(names)):\n",
    "            if term in document_t[document_m]:\n",
    "                document1.append(names[document_m])\n",
    "                document2.append(document_m + 1)\n",
    "        inverted_index[term] = document1\n",
    "        inverted_index2[term] = [len(set(document2)), ' - '.join([str(num) for num in document2])]\n",
    "    df = pd.DataFrame(inverted_index2)\n",
    "    df = df.transpose()\n",
    "    df.columns = ['Document Frequency', 'Postings Lists']\n",
    "    df.index.name = 'Term'\n",
    "    return dict(sorted(inverted_index.items())), df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99da7efa-9b67-4d83-87b2-5f82d935fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "document1 = \"The sky is blue.\"\n",
    "document2 = \"Blueberries are delicious.\"\n",
    "document3 = \"The blue car is parked outside.\"\n",
    "document4 = \"She wore a beautiful blue dress.\"\n",
    "document5 = \"The ocean sparkled with a brilliant blue hue.\"\n",
    "result,df=InvertedIndex({'document1': document1}, {'document2': document2},  \n",
    "              {'document3': document3}, {'document4': document4}, \n",
    "              {'document5': document5}, text_flage=1, tokenize_flage=1, bool_apply_stopword=1, bool_apply_stemming=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91a9161b-bda0-4834-bf3b-b97ee1bee909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". -> document1, document2, document3, document4, document5\n",
      "Blueberries -> document2\n",
      "She -> document4\n",
      "The -> document1, document3, document5\n",
      "beautiful -> document4\n",
      "blue -> document1, document3, document4, document5\n",
      "brilliant -> document5\n",
      "car -> document3\n",
      "delicious -> document2\n",
      "dress -> document4\n",
      "hue -> document5\n",
      "ocean -> document5\n",
      "outside -> document3\n",
      "parked -> document3\n",
      "sky -> document1\n",
      "sparkled -> document5\n",
      "wore -> document4\n"
     ]
    }
   ],
   "source": [
    "for term, documents in result.items():\n",
    "    print(term, \"->\", \", \".join(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f09d80c5-4d13-4225-8196-a9062cf9b903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document Frequency</th>\n",
       "      <th>Postings Lists</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>5</td>\n",
       "      <td>1 - 2 - 3 - 4 - 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blueberries</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>She</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The</th>\n",
       "      <td>3</td>\n",
       "      <td>1 - 3 - 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beautiful</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>4</td>\n",
       "      <td>1 - 3 - 4 - 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brilliant</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delicious</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dress</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hue</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ocean</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outside</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parked</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sky</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sparkled</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wore</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Document Frequency     Postings Lists\n",
       "Term                                             \n",
       ".                            5  1 - 2 - 3 - 4 - 5\n",
       "Blueberries                  1                  2\n",
       "She                          1                  4\n",
       "The                          3          1 - 3 - 5\n",
       "beautiful                    1                  4\n",
       "blue                         4      1 - 3 - 4 - 5\n",
       "brilliant                    1                  5\n",
       "car                          1                  3\n",
       "delicious                    1                  2\n",
       "dress                        1                  4\n",
       "hue                          1                  5\n",
       "ocean                        1                  5\n",
       "outside                      1                  3\n",
       "parked                       1                  3\n",
       "sky                          1                  1\n",
       "sparkled                     1                  5\n",
       "wore                         1                  4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d76b1d5-14a3-41d9-993a-791e679e1f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "document1 = \"الطالب يذاكر في المكتبة.\"\n",
    "document2 = \"الطالبة تكتب في الصفحة.\"\n",
    "document3 = \"السماء زرقاء اليوم.\"\n",
    "document4 = \"الأطفال يلعبون في الحديقة.\"\n",
    "document5 = \"الشمس تشرق في الصباح الباكر.\"\n",
    "result,df=InvertedIndex({'document1': document1}, {'document2': document2},  \n",
    "              {'document3': document3}, {'document4': document4}, \n",
    "              {'document5': document5}, text_flage=0, tokenize_flage=1, bool_apply_stopword=1, bool_apply_stemming=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9f6139a-a193-4f76-bcb5-8a1a61f28bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". -> document1, document2, document3, document4, document5\n",
      "الأطفال -> document4\n",
      "الباكر -> document5\n",
      "الحديقة -> document4\n",
      "السماء -> document3\n",
      "الشمس -> document5\n",
      "الصباح -> document5\n",
      "الصفحة -> document2\n",
      "الطالب -> document1\n",
      "الطالبة -> document2\n",
      "المكتبة -> document1\n",
      "اليوم -> document3\n",
      "تشرق -> document5\n",
      "تكتب -> document2\n",
      "زرقاء -> document3\n",
      "يذاكر -> document1\n",
      "يلعبون -> document4\n"
     ]
    }
   ],
   "source": [
    "for term, documents in result.items():\n",
    "    print(term, \"->\", \", \".join(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bb68e91-2aab-4a87-b3d3-9511d38f347c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document Frequency</th>\n",
       "      <th>Postings Lists</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>5</td>\n",
       "      <td>1 - 2 - 3 - 4 - 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>الأطفال</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>الباكر</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>الحديقة</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>السماء</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>الشمس</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>الصباح</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>الصفحة</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>الطالب</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>الطالبة</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>المكتبة</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>اليوم</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>تشرق</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>تكتب</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>زرقاء</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>يذاكر</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>يلعبون</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Document Frequency     Postings Lists\n",
       "Term                                         \n",
       ".                        5  1 - 2 - 3 - 4 - 5\n",
       "الأطفال                  1                  4\n",
       "الباكر                   1                  5\n",
       "الحديقة                  1                  4\n",
       "السماء                   1                  3\n",
       "الشمس                    1                  5\n",
       "الصباح                   1                  5\n",
       "الصفحة                   1                  2\n",
       "الطالب                   1                  1\n",
       "الطالبة                  1                  2\n",
       "المكتبة                  1                  1\n",
       "اليوم                    1                  3\n",
       "تشرق                     1                  5\n",
       "تكتب                     1                  2\n",
       "زرقاء                    1                  3\n",
       "يذاكر                    1                  1\n",
       "يلعبون                   1                  4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d9f1f4-bdd9-4eae-b7c1-8c531af253e4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### Leader Best Work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
